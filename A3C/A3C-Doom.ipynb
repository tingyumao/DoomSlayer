{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Reinforcement Learning with Tensorflow Part 8: Asynchronus Advantage Actor-Critic (A3C)\n",
    "\n",
    "This iPython notebook includes an implementation of the [A3C algorithm](https://arxiv.org/pdf/1602.01783.pdf). In it we use A3C to solve a simple 3D Doom challenge using the [VizDoom engine](http://vizdoom.cs.put.edu.pl/). For more information on A3C, see the accompanying [Medium post](https://medium.com/p/c88f72a5e9f2/edit).\n",
    "\n",
    "This tutorial requires that VizDoom is installed. It can be easily obtained with:\n",
    "\n",
    "`pip install vizdoom`\n",
    "\n",
    "We also require `basic.wad` and `helper.py`, both of which are available from the [DeepRL-Agents github repo](https://github.com/awjuliani/DeepRL-Agents).\n",
    "\n",
    "While training is taking place, statistics on agent performance are available from Tensorboard. To launch it use:\n",
    "\n",
    "`tensorboard --logdir=worker_0:'./train_0',worker_1:'./train_1',worker_2:'./train_2',worker_3:'./train_3'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tingyumao/Programs/anaconda3/envs/dlenv/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import threading\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import scipy.signal\n",
    "%matplotlib inline\n",
    "from helper import *\n",
    "from vizdoom import *\n",
    "\n",
    "from random import choice\n",
    "from time import sleep\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copies one set of variables to another.\n",
    "# Used to set worker network parameters to those of global network.\n",
    "def update_target_graph(from_scope,to_scope):\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "\n",
    "    op_holder = []\n",
    "    for from_var,to_var in zip(from_vars,to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder\n",
    "\n",
    "# Processes Doom screen image to produce cropped and resized image. \n",
    "def process_frame(frame):\n",
    "    s = frame[10:-10,30:-30]\n",
    "    s = scipy.misc.imresize(s,[84,84])\n",
    "    s = np.reshape(s,[np.prod(s.shape)]) / 255.0\n",
    "    return s\n",
    "\n",
    "# Discounting function used to calculate discounted returns.\n",
    "def discount(x, gamma):\n",
    "    return scipy.signal.lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]\n",
    "\n",
    "#Used to initialize weights for policy and value output layers\n",
    "def normalized_columns_initializer(std=1.0):\n",
    "    def _initializer(shape, dtype=None, partition_info=None):\n",
    "        out = np.random.randn(*shape).astype(np.float32)\n",
    "        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "        return tf.constant(out)\n",
    "    return _initializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor-Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AC_Network():\n",
    "    def __init__(self,s_size,a_size,scope,trainer):\n",
    "        with tf.variable_scope(scope):\n",
    "            #Input and visual encoding layers\n",
    "            self.inputs = tf.placeholder(shape=[None,s_size],dtype=tf.float32)\n",
    "            self.imageIn = tf.reshape(self.inputs,shape=[-1,84,84,1])\n",
    "            self.conv1 = slim.conv2d(activation_fn=tf.nn.elu,\n",
    "                inputs=self.imageIn,num_outputs=16,\n",
    "                kernel_size=[8,8],stride=[4,4],padding='VALID')\n",
    "            self.conv2 = slim.conv2d(activation_fn=tf.nn.elu,\n",
    "                inputs=self.conv1,num_outputs=32,\n",
    "                kernel_size=[4,4],stride=[2,2],padding='VALID')\n",
    "            hidden = slim.fully_connected(slim.flatten(self.conv2),256,activation_fn=tf.nn.elu)\n",
    "            \n",
    "            #Recurrent network for temporal dependencies\n",
    "            lstm_cell = tf.contrib.rnn.BasicLSTMCell(256,state_is_tuple=True)\n",
    "            c_init = np.zeros((1, lstm_cell.state_size.c), np.float32)\n",
    "            h_init = np.zeros((1, lstm_cell.state_size.h), np.float32)\n",
    "            self.state_init = [c_init, h_init]\n",
    "            c_in = tf.placeholder(tf.float32, [1, lstm_cell.state_size.c])\n",
    "            h_in = tf.placeholder(tf.float32, [1, lstm_cell.state_size.h])\n",
    "            self.state_in = (c_in, h_in)\n",
    "            rnn_in = tf.expand_dims(hidden, [0])\n",
    "            step_size = tf.shape(self.imageIn)[:1]\n",
    "            state_in = tf.contrib.rnn.LSTMStateTuple(c_in, h_in)\n",
    "            lstm_outputs, lstm_state = tf.nn.dynamic_rnn(\n",
    "                lstm_cell, rnn_in, initial_state=state_in, sequence_length=step_size,\n",
    "                time_major=False)\n",
    "            lstm_c, lstm_h = lstm_state\n",
    "            self.state_out = (lstm_c[:1, :], lstm_h[:1, :])\n",
    "            rnn_out = tf.reshape(lstm_outputs, [-1, 256])\n",
    "            \n",
    "            #Output layers for policy and value estimations\n",
    "            self.policy = slim.fully_connected(rnn_out,a_size,\n",
    "                activation_fn=tf.nn.softmax,\n",
    "                weights_initializer=normalized_columns_initializer(0.01),\n",
    "                biases_initializer=None)\n",
    "            self.value = slim.fully_connected(rnn_out,1,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=normalized_columns_initializer(1.0),\n",
    "                biases_initializer=None)\n",
    "            \n",
    "            #Only the worker network need ops for loss functions and gradient updating.\n",
    "            if scope != 'global':\n",
    "                self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "                self.actions_onehot = tf.one_hot(self.actions,a_size,dtype=tf.float32)\n",
    "                self.target_v = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "                self.advantages = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "\n",
    "                self.responsible_outputs = tf.reduce_sum(self.policy * self.actions_onehot, [1])\n",
    "\n",
    "                #Loss functions\n",
    "                self.value_loss = 0.5 * tf.reduce_sum(tf.square(self.target_v - tf.reshape(self.value,[-1])))\n",
    "                self.entropy = - tf.reduce_sum(self.policy * tf.log(self.policy))\n",
    "                self.policy_loss = -tf.reduce_sum(tf.log(self.responsible_outputs)*self.advantages)\n",
    "                self.loss = 0.5 * self.value_loss + self.policy_loss - self.entropy * 0.01\n",
    "\n",
    "                #Get gradients from local network using local losses\n",
    "                local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n",
    "                self.gradients = tf.gradients(self.loss,local_vars)\n",
    "                self.var_norms = tf.global_norm(local_vars)\n",
    "                grads,self.grad_norms = tf.clip_by_global_norm(self.gradients,40.0)\n",
    "                \n",
    "                #Apply local gradients to global network\n",
    "                global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'global')\n",
    "                self.apply_grads = trainer.apply_gradients(zip(grads,global_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worker Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker():\n",
    "    def __init__(self,game,name,s_size,a_size,trainer,model_path,global_episodes):\n",
    "        self.name = \"worker_\" + str(name)\n",
    "        self.number = name        \n",
    "        self.model_path = model_path\n",
    "        self.trainer = trainer\n",
    "        self.global_episodes = global_episodes\n",
    "        self.increment = self.global_episodes.assign_add(1)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_mean_values = []\n",
    "        self.summary_writer = tf.summary.FileWriter(\"train_\"+str(self.number))\n",
    "\n",
    "        #Create the local copy of the network and the tensorflow op to copy global paramters to local network\n",
    "        self.local_AC = AC_Network(s_size,a_size,self.name,trainer)\n",
    "        self.update_local_ops = update_target_graph('global',self.name)        \n",
    "        \n",
    "        #The Below code is related to setting up the Doom environment\n",
    "        game.set_doom_scenario_path(\"basic.wad\") #This corresponds to the simple task we will pose our agent\n",
    "        game.set_doom_map(\"map01\")\n",
    "        game.set_screen_resolution(ScreenResolution.RES_160X120)\n",
    "        game.set_screen_format(ScreenFormat.GRAY8)\n",
    "        game.set_render_hud(False)\n",
    "        game.set_render_crosshair(False)\n",
    "        game.set_render_weapon(True)\n",
    "        game.set_render_decals(False)\n",
    "        game.set_render_particles(False)\n",
    "        game.add_available_button(Button.MOVE_LEFT)\n",
    "        game.add_available_button(Button.MOVE_RIGHT)\n",
    "        game.add_available_button(Button.ATTACK)\n",
    "        game.add_available_game_variable(GameVariable.AMMO2)\n",
    "        game.add_available_game_variable(GameVariable.POSITION_X)\n",
    "        game.add_available_game_variable(GameVariable.POSITION_Y)\n",
    "        game.set_episode_timeout(300)\n",
    "        game.set_episode_start_time(10)\n",
    "        game.set_window_visible(False)\n",
    "        game.set_sound_enabled(False)\n",
    "        game.set_living_reward(-1)\n",
    "        game.set_mode(Mode.PLAYER)\n",
    "        game.init()\n",
    "        self.actions = self.actions = np.identity(a_size,dtype=bool).tolist()\n",
    "        #End Doom set-up\n",
    "        self.env = game\n",
    "        \n",
    "    def train(self,rollout,sess,gamma,bootstrap_value):\n",
    "        rollout = np.array(rollout)\n",
    "        observations = rollout[:,0]\n",
    "        actions = rollout[:,1]\n",
    "        rewards = rollout[:,2]\n",
    "        next_observations = rollout[:,3]\n",
    "        values = rollout[:,5]\n",
    "        \n",
    "        # Here we take the rewards and values from the rollout, and use them to \n",
    "        # generate the advantage and discounted returns. \n",
    "        # The advantage function uses \"Generalized Advantage Estimation\"\n",
    "        self.rewards_plus = np.asarray(rewards.tolist() + [bootstrap_value])\n",
    "        discounted_rewards = discount(self.rewards_plus,gamma)[:-1]\n",
    "        self.value_plus = np.asarray(values.tolist() + [bootstrap_value])\n",
    "        advantages = rewards + gamma * self.value_plus[1:] - self.value_plus[:-1]\n",
    "        advantages = discount(advantages,gamma)\n",
    "\n",
    "        # Update the global network using gradients from loss\n",
    "        # Generate network statistics to periodically save\n",
    "        feed_dict = {self.local_AC.target_v:discounted_rewards,\n",
    "            self.local_AC.inputs:np.vstack(observations),\n",
    "            self.local_AC.actions:actions,\n",
    "            self.local_AC.advantages:advantages,\n",
    "            self.local_AC.state_in[0]:self.batch_rnn_state[0],\n",
    "            self.local_AC.state_in[1]:self.batch_rnn_state[1]}\n",
    "        v_l,p_l,e_l,g_n,v_n, self.batch_rnn_state,_ = sess.run([self.local_AC.value_loss,\n",
    "            self.local_AC.policy_loss,\n",
    "            self.local_AC.entropy,\n",
    "            self.local_AC.grad_norms,\n",
    "            self.local_AC.var_norms,\n",
    "            self.local_AC.state_out,\n",
    "            self.local_AC.apply_grads],\n",
    "            feed_dict=feed_dict)\n",
    "        return v_l / len(rollout),p_l / len(rollout),e_l / len(rollout), g_n,v_n\n",
    "        \n",
    "    def work(self,max_episode_length,gamma,sess,coord,saver):\n",
    "        episode_count = sess.run(self.global_episodes)\n",
    "        total_steps = 0\n",
    "        print (\"Starting worker \" + str(self.number))\n",
    "        with sess.as_default(), sess.graph.as_default():                 \n",
    "            while not coord.should_stop():\n",
    "                sess.run(self.update_local_ops)\n",
    "                episode_buffer = []\n",
    "                episode_values = []\n",
    "                episode_frames = []\n",
    "                episode_reward = 0\n",
    "                episode_step_count = 0\n",
    "                d = False\n",
    "                \n",
    "                self.env.new_episode()\n",
    "                s = self.env.get_state().screen_buffer\n",
    "                episode_frames.append(s)\n",
    "                s = process_frame(s)\n",
    "                rnn_state = self.local_AC.state_init\n",
    "                self.batch_rnn_state = rnn_state\n",
    "                while self.env.is_episode_finished() == False:\n",
    "                    #Take an action using probabilities from policy network output.\n",
    "                    a_dist,v,rnn_state = sess.run([self.local_AC.policy,self.local_AC.value,self.local_AC.state_out], \n",
    "                        feed_dict={self.local_AC.inputs:[s],\n",
    "                        self.local_AC.state_in[0]:rnn_state[0],\n",
    "                        self.local_AC.state_in[1]:rnn_state[1]})\n",
    "                    a = np.random.choice(a_dist[0],p=a_dist[0])\n",
    "                    a = np.argmax(a_dist == a)\n",
    "\n",
    "                    r = self.env.make_action(self.actions[a]) / 100.0\n",
    "                    d = self.env.is_episode_finished()\n",
    "                    if d == False:\n",
    "                        s1 = self.env.get_state().screen_buffer\n",
    "                        episode_frames.append(s1)\n",
    "                        s1 = process_frame(s1)\n",
    "                    else:\n",
    "                        s1 = s\n",
    "                        \n",
    "                    episode_buffer.append([s,a,r,s1,d,v[0,0]])\n",
    "                    episode_values.append(v[0,0])\n",
    "\n",
    "                    episode_reward += r\n",
    "                    s = s1                    \n",
    "                    total_steps += 1\n",
    "                    episode_step_count += 1\n",
    "                    \n",
    "                    # If the episode hasn't ended, but the experience buffer is full, then we\n",
    "                    # make an update step using that experience rollout.\n",
    "                    if len(episode_buffer) == 30 and d != True and episode_step_count != max_episode_length - 1:\n",
    "                        # Since we don't know what the true final return is, we \"bootstrap\" from our current\n",
    "                        # value estimation.\n",
    "                        v1 = sess.run(self.local_AC.value, \n",
    "                            feed_dict={self.local_AC.inputs:[s],\n",
    "                            self.local_AC.state_in[0]:rnn_state[0],\n",
    "                            self.local_AC.state_in[1]:rnn_state[1]})[0,0]\n",
    "                        v_l,p_l,e_l,g_n,v_n = self.train(episode_buffer,sess,gamma,v1)\n",
    "                        episode_buffer = []\n",
    "                        sess.run(self.update_local_ops)\n",
    "                    if d == True:\n",
    "                        break\n",
    "                #print(\"reward: {}, steps: {}, value: {}\".format(episode_reward, episode_step_count, np.mean(episode_values)) \n",
    "                self.episode_rewards.append(episode_reward)\n",
    "                self.episode_lengths.append(episode_step_count)\n",
    "                self.episode_mean_values.append(np.mean(episode_values))\n",
    "                print(\"worker: {}, reward: {}, steps: {}, value: {}\".format(self.number, episode_reward, episode_step_count, np.mean(episode_values)))\n",
    "                \n",
    "                # Update the network using the episode buffer at the end of the episode.\n",
    "                if len(episode_buffer) != 0:\n",
    "                    v_l,p_l,e_l,g_n,v_n = self.train(episode_buffer,sess,gamma,0.0)\n",
    "                                \n",
    "                    \n",
    "                # Periodically save gifs of episodes, model parameters, and summary statistics.\n",
    "                if episode_count % 5 == 0 and episode_count != 0:\n",
    "                    if self.name == 'worker_0' and episode_count % 25 == 0:\n",
    "                        time_per_step = 0.05\n",
    "                        images = np.array(episode_frames)\n",
    "                        #make_gif(images,'./frames/image'+str(episode_count)+'.gif',\n",
    "                        #    duration=len(images)*time_per_step,true_image=True,salience=False)\n",
    "                    if episode_count % 250 == 0 and self.name == 'worker_0':\n",
    "                        saver.save(sess,self.model_path+'/model-'+str(episode_count)+'.cptk')\n",
    "                        print (\"Saved Model\")\n",
    "\n",
    "                    mean_reward = np.mean(self.episode_rewards[-5:])\n",
    "                    mean_length = np.mean(self.episode_lengths[-5:])\n",
    "                    mean_value = np.mean(self.episode_mean_values[-5:])\n",
    "                    summary = tf.Summary()\n",
    "                    summary.value.add(tag='Perf/Reward', simple_value=float(mean_reward))\n",
    "                    summary.value.add(tag='Perf/Length', simple_value=float(mean_length))\n",
    "                    summary.value.add(tag='Perf/Value', simple_value=float(mean_value))\n",
    "                    summary.value.add(tag='Losses/Value Loss', simple_value=float(v_l))\n",
    "                    summary.value.add(tag='Losses/Policy Loss', simple_value=float(p_l))\n",
    "                    summary.value.add(tag='Losses/Entropy', simple_value=float(e_l))\n",
    "                    summary.value.add(tag='Losses/Grad Norm', simple_value=float(g_n))\n",
    "                    summary.value.add(tag='Losses/Var Norm', simple_value=float(v_n))\n",
    "                    self.summary_writer.add_summary(summary, episode_count)\n",
    "\n",
    "                    self.summary_writer.flush()\n",
    "                if self.name == 'worker_0':\n",
    "                    sess.run(self.increment)\n",
    "                episode_count += 1\n",
    "                \n",
    "                if episode_count == 120000:  # thread to stop\n",
    "                    print(\"Stop training name:{}\".format(self.name))\n",
    "                    coord.request_stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_episode_length = 300\n",
    "gamma = .99 # discount rate for advantage estimation and reward discounting\n",
    "s_size = 7056 # Observations are greyscale frames of 84 * 84 * 1\n",
    "a_size = 3 # Agent can move Left, Right, or Fire\n",
    "load_model = False\n",
    "model_path = './model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting worker 0\n",
      "reward: 0.6499999999999999, steps: 31, value: 0.04827013984322548\n",
      "Starting worker 1\n",
      "Starting worker 2\n",
      "Starting worker 3\n",
      "reward: -3.849999999999973, steps: 300, value: -0.031989745795726776\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.10997817665338516\n",
      "reward: 0.91, steps: 10, value: -0.8249842524528503\n",
      "reward: -3.8499999999999726, steps: 300, value: -0.4510517418384552\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.6244832873344421\n",
      "reward: -3.8499999999999726, steps: 300, value: -1.0411983728408813\n",
      "reward: -3.899999999999973, steps: 300, value: -1.0441807508468628\n",
      "reward: 0.91, steps: 10, value: -1.0586612224578857\n",
      "reward: -3.8499999999999726, steps: 300, value: -1.0754296779632568\n",
      "reward: 0.89, steps: 12, value: -0.7347631454467773\n",
      "reward: 0.5899999999999999, steps: 37, value: -0.6264020800590515\n",
      "reward: -3.7999999999999723, steps: 300, value: -1.0155881643295288\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.6676064133644104\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.6351053714752197\n",
      "reward: 0.85, steps: 16, value: -0.6506320834159851\n",
      "reward: -1.5099999999999933, steps: 202, value: -0.4941886365413666\n",
      "reward: 0.9, steps: 11, value: -0.5737823843955994\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.5201063752174377\n",
      "reward: -1.0999999999999994, steps: 171, value: -0.4959971010684967\n",
      "reward: -0.8900000000000015, steps: 150, value: -0.4954066872596741\n",
      "reward: 0.21999999999999953, steps: 64, value: -0.5179739594459534\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.5576635003089905\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.5594145655632019\n",
      "reward: 0.87, steps: 14, value: -0.6128724217414856\n",
      "reward: -0.7200000000000013, steps: 143, value: -0.6742991209030151\n",
      "reward: -3.8499999999999726, steps: 300, value: -0.6404642462730408\n",
      "reward: -2.0499999999999856, steps: 236, value: -0.6355366110801697\n",
      "reward: 0.91, steps: 10, value: -0.6534923315048218\n",
      "reward: -1.3999999999999944, steps: 196, value: -0.6559937000274658\n",
      "reward: -3.799999999999973, steps: 300, value: -0.6532223224639893\n",
      "reward: -0.010000000000000666, steps: 82, value: -0.7725451588630676\n",
      "reward: -3.7999999999999727, steps: 300, value: -0.7665135860443115\n",
      "reward: -3.7499999999999742, steps: 300, value: -0.7800923585891724\n",
      "reward: -3.849999999999973, steps: 300, value: -0.7929158806800842\n",
      "reward: -0.370000000000001, steps: 113, value: -0.8664312958717346\n",
      "reward: -3.8499999999999734, steps: 300, value: -0.8483145236968994\n",
      "reward: 0.6499999999999999, steps: 31, value: -1.0037912130355835\n",
      "reward: 0.71, steps: 25, value: -0.996102511882782\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.8937995433807373\n",
      "reward: -3.8999999999999733, steps: 300, value: -0.884421169757843\n",
      "reward: 0.89, steps: 12, value: -0.7210598587989807\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.8427751660346985\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.6196268200874329\n",
      "reward: -3.8499999999999726, steps: 300, value: -0.647797703742981\n",
      "reward: -3.8499999999999726, steps: 300, value: -0.6246398687362671\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.6475962996482849\n",
      "reward: 0.9, steps: 11, value: -0.748140275478363\n",
      "reward: -3.749999999999972, steps: 300, value: -0.8300718069076538\n",
      "reward: -3.8499999999999726, steps: 300, value: -0.8555525541305542\n",
      "reward: -3.8499999999999726, steps: 300, value: -0.8515414595603943\n",
      "reward: 0.91, steps: 10, value: -0.8361358642578125\n",
      "reward: 0.89, steps: 12, value: -0.9026658535003662\n",
      "reward: -3.8499999999999726, steps: 300, value: -0.8477138876914978\n",
      "reward: -0.040000000000000695, steps: 85, value: -0.5918607711791992\n",
      "reward: -3.8499999999999726, steps: 300, value: -0.6813791990280151\n",
      "reward: -2.1899999999999826, steps: 255, value: -0.5718101859092712\n",
      "reward: -3.799999999999973, steps: 300, value: -0.572127640247345\n",
      "reward: -3.8499999999999726, steps: 300, value: -0.5992948412895203\n",
      "reward: 0.6299999999999999, steps: 33, value: -0.8445632457733154\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.7638506889343262\n",
      "reward: -3.799999999999973, steps: 300, value: -0.8125577569007874\n",
      "reward: 0.91, steps: 10, value: -0.7751691937446594\n",
      "reward: -3.899999999999973, steps: 300, value: -0.8719270825386047\n",
      "reward: 0.31999999999999973, steps: 59, value: -0.9715461730957031\n",
      "reward: -3.849999999999973, steps: 300, value: -0.7757391929626465\n",
      "reward: -1.6799999999999897, steps: 214, value: -0.5935908555984497\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.6563600897789001\n",
      "reward: 0.91, steps: 10, value: -0.7902083396911621\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.6680119037628174\n",
      "reward: 0.9, steps: 11, value: -0.7880096435546875\n",
      "reward: 0.7199999999999999, steps: 24, value: -0.7634907364845276\n",
      "reward: 0.9, steps: 11, value: -0.5587084293365479\n",
      "reward: 0.71, steps: 25, value: -0.532192587852478\n",
      "reward: 0.20999999999999952, steps: 65, value: -0.39247483015060425\n",
      "reward: -1.8499999999999874, steps: 231, value: -0.6291974782943726\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.6186127662658691\n",
      "reward: 0.4199999999999998, steps: 49, value: -0.30050429701805115\n",
      "reward: -0.34000000000000097, steps: 105, value: -0.31366562843322754\n",
      "reward: 0.9, steps: 11, value: -0.258046418428421\n",
      "reward: -3.8499999999999726, steps: 300, value: -0.4277580678462982\n",
      "reward: 0.12999999999999945, steps: 73, value: -0.3226318955421448\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.33994755148887634\n",
      "reward: 0.08999999999999943, steps: 77, value: -0.4223407506942749\n",
      "reward: -3.8499999999999726, steps: 300, value: -0.3920450210571289\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.42331039905548096\n",
      "reward: -2.359999999999979, steps: 267, value: -0.6422313451766968\n",
      "reward: -3.7999999999999727, steps: 300, value: -0.6507766246795654\n",
      "reward: 0.91, steps: 10, value: -0.8345988392829895\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.7900047898292542\n",
      "reward: 0.9, steps: 11, value: -0.6208932399749756\n",
      "reward: 0.6099999999999999, steps: 35, value: -0.6396650671958923\n",
      "reward: -3.799999999999973, steps: 300, value: -0.8045609593391418\n",
      "reward: 0.9, steps: 11, value: -0.5055539608001709\n",
      "reward: -0.2500000000000009, steps: 101, value: -0.522962749004364\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.699776291847229\n",
      "reward: -3.8499999999999726, steps: 300, value: -0.6525208353996277\n",
      "reward: -3.8499999999999726, steps: 300, value: -0.5560626983642578\n",
      "reward: -2.3399999999999808, steps: 265, value: -0.5949002504348755\n",
      "reward: 0.6299999999999999, steps: 33, value: -0.712171733379364\n",
      "reward: 0.87, steps: 14, value: -0.6382085084915161\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.6462108492851257\n",
      "reward: -3.799999999999973, steps: 300, value: -0.662428617477417\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.6809288263320923\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.6984584927558899\n",
      "reward: -3.849999999999973, steps: 300, value: -0.7113091945648193\n",
      "reward: 0.9, steps: 11, value: -0.6964266300201416\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.7313367128372192\n",
      "reward: -1.889999999999988, steps: 230, value: -0.762220561504364\n",
      "reward: -1.7399999999999898, steps: 215, value: -0.60907381772995\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.6725651025772095\n",
      "reward: -3.8499999999999726, steps: 300, value: -0.6507314443588257\n",
      "reward: -3.899999999999973, steps: 300, value: -0.663933515548706\n",
      "reward: 0.44999999999999973, steps: 46, value: -0.7598820328712463\n",
      "reward: 0.9, steps: 11, value: -0.6523557305335999\n",
      "reward: -3.849999999999974, steps: 300, value: -0.7334379553794861\n",
      "reward: -3.849999999999974, steps: 300, value: -0.7324779033660889\n",
      "reward: -3.8999999999999715, steps: 300, value: -0.7226058840751648\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.6725881695747375\n",
      "reward: -2.009999999999985, steps: 237, value: -0.6766991019248962\n",
      "reward: -1.119999999999999, steps: 168, value: -0.6973027586936951\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.6882075071334839\n",
      "reward: -2.7199999999999727, steps: 298, value: -0.6598137021064758\n",
      "reward: -3.799999999999973, steps: 300, value: -0.6743078231811523\n",
      "reward: -3.8499999999999726, steps: 300, value: -0.6655097603797913\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.6816164255142212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: -1.6499999999999904, steps: 211, value: -0.6766137480735779\n",
      "reward: -3.799999999999973, steps: 300, value: -0.7200749516487122\n",
      "reward: -1.9999999999999856, steps: 236, value: -0.6790274977684021\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.7123327255249023\n",
      "reward: 0.91, steps: 10, value: -0.5650015473365784\n",
      "reward: 0.10999999999999967, steps: 75, value: -0.6551291942596436\n",
      "reward: -3.8499999999999726, steps: 300, value: -0.5638812184333801\n",
      "reward: -1.6399999999999906, steps: 210, value: -0.49375125765800476\n",
      "reward: -3.699999999999974, steps: 300, value: -0.5477348566055298\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.5023844242095947\n",
      "reward: 0.91, steps: 10, value: -0.5033084750175476\n",
      "reward: -3.7999999999999727, steps: 300, value: -0.6018075346946716\n",
      "reward: -3.8499999999999734, steps: 300, value: -0.6168119311332703\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.6034820079803467\n",
      "reward: 0.85, steps: 16, value: -0.7137086391448975\n",
      "reward: 0.12999999999999945, steps: 73, value: -0.763434112071991\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.6248222589492798\n",
      "reward: -0.7600000000000011, steps: 142, value: -0.5491958260536194\n",
      "reward: -1.7399999999999884, steps: 220, value: -0.5115869045257568\n",
      "reward: -0.5600000000000012, steps: 127, value: -0.513565182685852\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.5393827557563782\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.5624405741691589\n",
      "reward: -0.6200000000000012, steps: 133, value: -0.5812919735908508\n",
      "reward: 0.1599999999999997, steps: 70, value: -0.5917944312095642\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.6200646758079529\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.6430084109306335\n",
      "reward: -3.849999999999974, steps: 300, value: -0.6577697992324829\n",
      "reward: -0.12000000000000054, steps: 93, value: -0.7514936923980713\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.7338280081748962\n",
      "reward: -2.259999999999981, steps: 262, value: -0.7523134350776672\n",
      "reward: -3.849999999999973, steps: 300, value: -0.7459650635719299\n",
      "reward: -0.2600000000000009, steps: 102, value: -0.8183238506317139\n",
      "reward: 0.91, steps: 10, value: -0.7282152771949768\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.7589865326881409\n",
      "reward: -0.6100000000000012, steps: 127, value: -0.8086537718772888\n",
      "reward: 0.89, steps: 12, value: -0.6343087553977966\n",
      "reward: -3.7499999999999725, steps: 300, value: -0.7802070379257202\n",
      "reward: 0.47999999999999976, steps: 43, value: -0.5678098201751709\n",
      "reward: 0.039999999999999376, steps: 77, value: -0.5461668372154236\n",
      "reward: -1.2699999999999971, steps: 178, value: -0.4896277189254761\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.46444466710090637\n",
      "reward: -0.1600000000000008, steps: 97, value: -0.35823941230773926\n",
      "reward: 0.91, steps: 10, value: -0.4031847417354584\n",
      "reward: 0.91, steps: 10, value: -0.40722519159317017\n",
      "reward: 0.91, steps: 10, value: -0.3927895128726959\n",
      "reward: 0.44999999999999973, steps: 46, value: -0.42179203033447266\n",
      "reward: 0.91, steps: 10, value: -0.3754085302352905\n",
      "reward: 0.86, steps: 15, value: -0.3699473440647125\n",
      "reward: -3.7499999999999742, steps: 300, value: -0.360372930765152\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.36308911442756653\n",
      "reward: 0.9, steps: 11, value: -0.23649384081363678\n",
      "reward: -0.17000000000000082, steps: 93, value: -0.3046903610229492\n",
      "reward: 0.7199999999999999, steps: 24, value: -0.23388363420963287\n",
      "reward: 0.7199999999999999, steps: 24, value: -0.2160804718732834\n",
      "reward: -0.5700000000000012, steps: 128, value: -0.24691571295261383\n",
      "reward: 0.21999999999999953, steps: 64, value: -0.12250397354364395\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.2083161622285843\n",
      "reward: 0.6499999999999999, steps: 31, value: -0.39342617988586426\n",
      "reward: -3.8499999999999726, steps: 300, value: -0.2603384256362915\n",
      "reward: 0.91, steps: 10, value: -0.36148303747177124\n",
      "reward: -3.849999999999973, steps: 300, value: -0.2724408507347107\n",
      "reward: -3.8499999999999734, steps: 300, value: -0.3784424960613251\n",
      "reward: -3.799999999999973, steps: 300, value: -0.5894439220428467\n",
      "reward: -3.8499999999999726, steps: 300, value: -0.6427606344223022\n",
      "reward: 0.6299999999999998, steps: 33, value: -0.9265114068984985\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.6759668588638306\n",
      "reward: 0.82, steps: 19, value: -0.7177107930183411\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.7570359110832214\n",
      "reward: -0.4600000000000011, steps: 117, value: -0.7102794051170349\n",
      "reward: 0.009999999999999351, steps: 80, value: -0.4783121943473816\n",
      "reward: -0.5500000000000012, steps: 126, value: -0.4831754267215729\n",
      "reward: -0.010000000000000444, steps: 82, value: -0.4551534056663513\n",
      "reward: -3.7999999999999727, steps: 300, value: -0.5311927199363708\n",
      "reward: 0.89, steps: 12, value: -0.3652399778366089\n",
      "reward: 0.9, steps: 11, value: -0.35561180114746094\n",
      "reward: -3.799999999999974, steps: 300, value: -0.4885117709636688\n",
      "reward: -0.5200000000000011, steps: 123, value: -0.45264583826065063\n",
      "reward: -0.09000000000000073, steps: 90, value: -0.43352043628692627\n",
      "reward: 0.7199999999999999, steps: 24, value: -0.306608647108078\n",
      "reward: -3.849999999999974, steps: 300, value: -0.35995012521743774\n",
      "reward: -3.7499999999999742, steps: 300, value: -0.3699510097503662\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.3673367202281952\n",
      "reward: 0.45999999999999974, steps: 45, value: -0.5053149461746216\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.3730145990848541\n",
      "reward: -0.20000000000000062, steps: 101, value: -0.5221370458602905\n",
      "reward: -0.20000000000000084, steps: 101, value: -0.5021336078643799\n",
      "reward: 0.88, steps: 13, value: -0.43790557980537415\n",
      "reward: 0.4299999999999997, steps: 48, value: -0.4567340314388275\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.4475974440574646\n",
      "reward: -0.9200000000000015, steps: 148, value: -0.4724593162536621\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.4838186502456665\n",
      "reward: 0.69, steps: 27, value: -0.5352298617362976\n",
      "reward: 0.9, steps: 11, value: -0.453743577003479\n",
      "reward: 0.89, steps: 12, value: -0.42718589305877686\n",
      "reward: -3.7999999999999727, steps: 300, value: -0.4661642909049988\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.419546902179718\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.4509193003177643\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.4307514429092407\n",
      "reward: 0.20999999999999952, steps: 65, value: -0.6080107688903809\n",
      "reward: 0.89, steps: 12, value: -0.5569248199462891\n",
      "reward: 0.91, steps: 10, value: -0.5353030562400818\n",
      "reward: -3.7999999999999727, steps: 300, value: -0.4695090651512146\n",
      "reward: -0.8200000000000014, steps: 148, value: -0.5607115030288696\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.5907252430915833\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.5966863036155701\n",
      "reward: -3.8499999999999726, steps: 300, value: -0.5808365941047668\n",
      "reward: 0.04999999999999961, steps: 76, value: -0.6355846524238586\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.6130313873291016\n",
      "reward: 0.34999999999999976, steps: 56, value: -0.6662649512290955\n",
      "reward: -3.849999999999974, steps: 300, value: -0.6300369501113892\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.629464328289032\n",
      "reward: 0.6199999999999999, steps: 34, value: -0.6354175209999084\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.6323475241661072\n",
      "reward: -1.0000000000000016, steps: 156, value: -0.6295029520988464\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.6404085755348206\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.6488087177276611\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.6939513683319092\n",
      "reward: -0.1400000000000008, steps: 95, value: -0.7925407886505127\n",
      "reward: 0.88, steps: 13, value: -0.7313801050186157\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.7230266332626343\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.7356760501861572\n",
      "reward: 0.6599999999999998, steps: 30, value: -0.6814517378807068\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.6866474747657776\n",
      "reward: -0.2500000000000009, steps: 101, value: -0.6766109466552734\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.6785621047019958\n",
      "reward: 0.17999999999999972, steps: 68, value: -0.634880542755127\n",
      "reward: -3.799999999999973, steps: 300, value: -0.6493239402770996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 0.88, steps: 13, value: -0.5320727825164795\n",
      "reward: -0.10000000000000074, steps: 91, value: -0.6326844096183777\n",
      "reward: 0.2899999999999997, steps: 62, value: -0.5210972428321838reward: 0.36999999999999966, steps: 54, value: -0.5065023899078369\n",
      "\n",
      "reward: 0.44999999999999973, steps: 46, value: -0.4337916672229767\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.5112437605857849\n",
      "reward: -1.7199999999999902, steps: 213, value: -0.5141362547874451\n",
      "reward: 0.9, steps: 11, value: -0.3476879894733429\n",
      "reward: -0.7200000000000013, steps: 138, value: -0.4235938787460327\n",
      "reward: -0.6000000000000012, steps: 131, value: -0.40483105182647705\n",
      "reward: -0.8000000000000014, steps: 146, value: -0.4143754541873932\n",
      "reward: 0.82, steps: 19, value: -0.3777729868888855\n",
      "reward: -2.4799999999999778, steps: 279, value: -0.4217548370361328\n",
      "reward: -0.9600000000000015, steps: 157, value: -0.42644309997558594\n",
      "reward: -3.7499999999999747, steps: 300, value: -0.39916515350341797\n",
      "reward: -0.10000000000000074, steps: 91, value: -0.3892936706542969\n",
      "reward: -0.1800000000000006, steps: 94, value: -0.42105093598365784\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.4182189106941223\n",
      "reward: 0.21999999999999953, steps: 64, value: -0.5612116456031799\n",
      "reward: 0.87, steps: 14, value: -0.5627874135971069\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.4730147421360016\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.4913384020328522\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.5532999634742737\n",
      "reward: -0.5500000000000012, steps: 126, value: -0.6739144325256348\n",
      "reward: -3.799999999999973, steps: 300, value: -0.6935537457466125\n",
      "reward: -3.8499999999999726, steps: 300, value: -0.6977819800376892\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.7221338748931885\n",
      "reward: -3.749999999999972, steps: 300, value: -0.727944552898407\n",
      "reward: 0.12999999999999967, steps: 73, value: -0.8524205088615417\n",
      "reward: 0.7999999999999999, steps: 21, value: -0.7103683948516846\n",
      "reward: 0.44999999999999973, steps: 46, value: -0.6579021215438843\n",
      "reward: -0.5500000000000012, steps: 126, value: -0.4519467055797577\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.5433516502380371\n",
      "reward: 0.88, steps: 13, value: -0.43784570693969727\n",
      "reward: -3.8499999999999726, steps: 300, value: -0.48161637783050537\n",
      "reward: 0.91, steps: 10, value: -0.430338054895401\n",
      "reward: 0.6499999999999999, steps: 31, value: -0.5136087536811829\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.532518208026886\n",
      "reward: 0.2899999999999996, steps: 57, value: -0.4715915620326996\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.4589401185512543\n",
      "reward: -0.3900000000000008, steps: 110, value: -0.3998498320579529\n",
      "reward: 0.5999999999999999, steps: 36, value: -0.36601725220680237\n",
      "reward: 0.89, steps: 12, value: -0.3162442445755005\n",
      "reward: 0.37999999999999967, steps: 53, value: -0.36442407965660095\n",
      "reward: -3.899999999999973, steps: 300, value: -0.4234996438026428\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.4278261959552765\n",
      "reward: 0.17999999999999972, steps: 68, value: -0.37438473105430603\n",
      "reward: -1.479999999999994, steps: 199, value: -0.39003926515579224\n",
      "reward: -1.8899999999999866, steps: 230, value: -0.39987826347351074\n",
      "reward: 0.11999999999999945, steps: 74, value: -0.38125237822532654\n",
      "reward: 0.87, steps: 14, value: -0.3389113247394562\n",
      "reward: 0.6299999999999999, steps: 33, value: -0.4113551378250122\n",
      "reward: -0.07000000000000071, steps: 88, value: -0.4194064438343048\n",
      "reward: 0.36999999999999966, steps: 54, value: -0.33599478006362915\n",
      "reward: 0.84, steps: 17, value: -0.2437579184770584\n",
      "reward: -0.18000000000000083, steps: 99, value: -0.3529226779937744\n",
      "reward: -3.799999999999973, steps: 300, value: -0.35746002197265625\n",
      "reward: 0.21999999999999953, steps: 64, value: -0.2064172476530075\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.2919410467147827\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.2952253818511963\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.3514334261417389\n",
      "reward: -3.8499999999999726, steps: 300, value: -0.43479472398757935\n",
      "reward: -3.8499999999999726, steps: 300, value: -0.6442365050315857\n",
      "reward: 0.33999999999999964, steps: 57, value: -1.076642394065857\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.8011254668235779\n",
      "reward: -3.799999999999973, steps: 300, value: -0.8268052935600281\n",
      "reward: -0.030000000000000686, steps: 84, value: -0.8794562220573425\n",
      "reward: -3.8499999999999726, steps: 300, value: -0.8422823548316956\n",
      "reward: -0.21000000000000063, steps: 97, value: -0.7359778881072998\n",
      "reward: 0.6399999999999999, steps: 32, value: -0.5006000995635986\n",
      "reward: 0.1899999999999995, steps: 67, value: -0.43600523471832275\n",
      "reward: 0.6599999999999998, steps: 30, value: -0.38535797595977783\n",
      "reward: 0.2899999999999996, steps: 57, value: -0.36944466829299927\n",
      "reward: -3.7499999999999742, steps: 300, value: -0.4300835132598877\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.38527294993400574\n",
      "reward: 0.1799999999999995, steps: 68, value: -0.3477299213409424\n",
      "reward: 0.5199999999999998, steps: 39, value: -0.4089321792125702\n",
      "reward: -3.849999999999973, steps: 300, value: -0.3633641302585602\n",
      "reward: 0.6299999999999998, steps: 33, value: -0.4209359884262085\n",
      "reward: 0.6599999999999998, steps: 30, value: -0.44255155324935913\n",
      "reward: 0.2999999999999996, steps: 61, value: -0.4469604790210724\n",
      "reward: 0.6499999999999999, steps: 31, value: -0.44871145486831665\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.38678425550460815\n",
      "reward: 0.6699999999999998, steps: 29, value: -0.32904115319252014\n",
      "reward: -0.8500000000000014, steps: 151, value: -0.3197493851184845\n",
      "reward: 0.06999999999999941, steps: 79, value: -0.2943146526813507\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.3514672815799713\n",
      "reward: -1.279999999999997, steps: 184, value: -0.3253847658634186\n",
      "reward: -0.2600000000000009, steps: 102, value: -0.23025144636631012\n",
      "reward: 0.34999999999999964, steps: 56, value: -0.24460847675800323\n",
      "reward: 0.9, steps: 11, value: -0.1641601026058197\n",
      "reward: -3.7999999999999727, steps: 300, value: -0.311031699180603\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.32371786236763\n",
      "reward: 0.89, steps: 12, value: -0.32990649342536926\n",
      "reward: -2.31999999999998, steps: 263, value: -0.3546958863735199\n",
      "reward: -3.849999999999973, steps: 300, value: -0.3701431453227997\n",
      "reward: -0.2900000000000009, steps: 105, value: -0.4181700348854065\n",
      "reward: 0.45999999999999974, steps: 45, value: -0.3786555826663971\n",
      "reward: -0.18000000000000083, steps: 99, value: -0.40597259998321533\n",
      "reward: 0.1899999999999995, steps: 67, value: -0.352440744638443\n",
      "reward: 0.7199999999999999, steps: 24, value: -0.2770092189311981\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.32463815808296204\n",
      "reward: 0.20999999999999952, steps: 65, value: -0.5223013162612915\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.378193199634552\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.42120498418807983\n",
      "reward: 0.9, steps: 11, value: -0.5906000733375549\n",
      "reward: -3.849999999999974, steps: 300, value: -0.4361187815666199\n",
      "reward: 0.89, steps: 12, value: -0.4668690860271454\n",
      "reward: -3.849999999999974, steps: 300, value: -0.5856654047966003\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.5699693560600281\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.572530210018158\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.5738499164581299\n",
      "reward: -0.4800000000000011, steps: 119, value: -0.6617832779884338\n",
      "reward: 0.06999999999999941, steps: 79, value: -0.6309782266616821\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.6003907322883606\n",
      "reward: -3.8499999999999726, steps: 300, value: -0.5869722962379456\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.590262234210968\n",
      "reward: 0.6499999999999999, steps: 31, value: -0.6331762671470642\n",
      "reward: 0.85, steps: 16, value: -0.5864430069923401\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.6063628792762756\n",
      "reward: 0.6299999999999998, steps: 33, value: -0.6509531140327454\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.6616607308387756\n",
      "reward: 0.3899999999999998, steps: 52, value: -0.5777351260185242\n",
      "reward: 0.91, steps: 10, value: -0.4950069785118103\n",
      "reward: 0.9, steps: 11, value: -0.47045400738716125\n",
      "reward: -3.749999999999972, steps: 300, value: -0.6235678195953369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 0.88, steps: 13, value: -0.487798810005188\n",
      "reward: 0.029999999999999367, steps: 78, value: -0.5000832080841064\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.5754712820053101\n",
      "reward: -3.8499999999999726, steps: 300, value: -0.5505813360214233\n",
      "reward: 0.1799999999999995, steps: 68, value: -0.4238130748271942\n",
      "reward: 0.12999999999999945, steps: 73, value: -0.4142879545688629\n",
      "reward: -6.574601973952099e-16, steps: 81, value: -0.40352827310562134\n",
      "reward: -3.799999999999973, steps: 300, value: -0.4609556496143341\n",
      "reward: 0.89, steps: 12, value: -0.28633758425712585\n",
      "reward: 0.5899999999999999, steps: 37, value: -0.39801087975502014\n",
      "reward: 0.01999999999999936, steps: 79, value: -0.49695315957069397\n",
      "reward: -3.8499999999999726, steps: 300, value: -0.47348877787590027\n",
      "reward: 0.5999999999999999, steps: 36, value: -0.42885011434555054\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.4914466440677643\n",
      "reward: 0.36999999999999966, steps: 54, value: -0.4667801558971405\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.49809661507606506\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.5158370137214661\n",
      "reward: -3.8499999999999734, steps: 300, value: -0.5511772632598877\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.5395981073379517reward: 0.3999999999999998, steps: 51, value: -0.5550433993339539\n",
      "\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.5340865850448608\n",
      "reward: 0.6499999999999999, steps: 31, value: -0.5477330684661865\n",
      "reward: -0.23000000000000065, steps: 99, value: -0.4599635899066925\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.5443524122238159\n",
      "reward: 0.91, steps: 10, value: -0.4228612780570984\n",
      "reward: -3.6999999999999744, steps: 300, value: -0.5278923511505127\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.5425418019294739\n",
      "reward: 0.7999999999999999, steps: 21, value: -0.3705867826938629\n",
      "reward: 0.08999999999999965, steps: 77, value: -0.4385589063167572\n",
      "reward: -0.5900000000000012, steps: 130, value: -0.477519154548645\n",
      "reward: 0.89, steps: 12, value: -0.33784735202789307\n",
      "reward: -0.350000000000001, steps: 111, value: -0.4569634199142456\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.5571447014808655\n",
      "reward: 0.88, steps: 13, value: -0.23923903703689575\n",
      "reward: -0.21000000000000085, steps: 102, value: -0.2930293679237366\n",
      "reward: -0.2600000000000009, steps: 102, value: -0.27283766865730286\n",
      "reward: 0.6499999999999998, steps: 31, value: -0.1581660807132721\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.42988839745521545\n",
      "reward: 0.91, steps: 10, value: -0.13511376082897186\n",
      "reward: -3.699999999999973, steps: 300, value: -0.3963812589645386\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.4616630971431732\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.5312875509262085\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.6497021317481995\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.6931372880935669\n",
      "reward: 0.24999999999999956, steps: 61, value: -0.8779096603393555\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.782903254032135\n",
      "reward: 0.6299999999999999, steps: 33, value: -0.5372060537338257\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.7899036407470703\n",
      "reward: 0.47999999999999987, steps: 43, value: -0.5309495329856873\n",
      "reward: 0.5799999999999998, steps: 38, value: -0.49044182896614075\n",
      "reward: -0.6100000000000012, steps: 132, value: -0.548380434513092\n",
      "reward: 0.6699999999999998, steps: 29, value: -0.39106154441833496\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.6621882915496826\n",
      "reward: 0.5999999999999999, steps: 36, value: -0.33054476976394653\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.6086868047714233\n",
      "reward: 0.6099999999999999, steps: 35, value: -0.326249361038208\n",
      "reward: 0.87, steps: 14, value: -0.058413274586200714\n",
      "reward: 0.3999999999999997, steps: 51, value: -0.3772454261779785\n",
      "reward: 0.89, steps: 12, value: -0.08338382095098495\n",
      "reward: 0.3899999999999997, steps: 52, value: -0.15350085496902466\n",
      "reward: 0.47999999999999976, steps: 43, value: -0.0081033855676651\n",
      "reward: 0.6199999999999999, steps: 34, value: 0.26830989122390747\n",
      "reward: -3.7499999999999747, steps: 300, value: -0.40981435775756836\n",
      "reward: 0.6199999999999999, steps: 34, value: -0.043415892869234085\n",
      "reward: -0.040000000000000695, steps: 85, value: -0.24134795367717743\n",
      "reward: 0.6499999999999999, steps: 31, value: -0.10548777133226395\n",
      "reward: 0.5199999999999998, steps: 39, value: -0.1379699409008026\n",
      "reward: 0.44999999999999973, steps: 46, value: -0.24167048931121826\n",
      "reward: -3.899999999999973, steps: 300, value: -0.4032957851886749\n",
      "reward: -3.7499999999999742, steps: 300, value: -0.41281482577323914\n",
      "reward: -3.749999999999972, steps: 300, value: -0.534545361995697\n",
      "reward: -3.899999999999973, steps: 300, value: -0.5456339120864868\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.5583198070526123\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.5772511959075928\n",
      "reward: 0.22999999999999954, steps: 63, value: -0.3719542324542999\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.6596789360046387\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.6638049483299255\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.6816219091415405\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.7056258916854858\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.7343440651893616\n",
      "reward: 0.89, steps: 12, value: -0.3343396484851837\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.7598666548728943\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.7729486227035522\n",
      "reward: 0.1799999999999995, steps: 68, value: -0.7128942608833313\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.7655887007713318\n",
      "reward: -6.574601973952099e-16, steps: 81, value: -0.5076426863670349\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.6185013651847839\n",
      "reward: -3.6999999999999744, steps: 300, value: -0.6080206036567688\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.6008883118629456\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.6313921809196472\n",
      "reward: 0.4199999999999997, steps: 49, value: -0.5340714454650879\n",
      "reward: 0.16999999999999948, steps: 69, value: -0.5851154923439026\n",
      "reward: 0.89, steps: 12, value: -0.18277813494205475\n",
      "reward: -0.1500000000000008, steps: 96, value: -0.35179004073143005\n",
      "reward: -0.2600000000000009, steps: 102, value: -0.39926597476005554\n",
      "reward: -3.7499999999999747, steps: 300, value: -0.6280949115753174\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.5600858330726624\n",
      "reward: -3.699999999999973, steps: 300, value: -0.629971981048584\n",
      "reward: -3.699999999999974, steps: 300, value: -0.6065579056739807\n",
      "reward: 0.91, steps: 10, value: -0.3373414874076843\n",
      "reward: 0.89, steps: 12, value: -0.0014950074255466461\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.6715186834335327\n",
      "reward: 0.07999999999999942, steps: 78, value: -0.24128392338752747\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.6580954194068909\n",
      "reward: 0.91, steps: 10, value: 0.32945796847343445\n",
      "reward: 0.6299999999999999, steps: 33, value: -0.1290156990289688\n",
      "reward: 0.91, steps: 10, value: 0.27974003553390503\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.6439027190208435\n",
      "reward: 0.36999999999999966, steps: 54, value: -0.428654283285141\n",
      "reward: -3.6999999999999744, steps: 300, value: -0.6557052135467529\n",
      "reward: 0.5899999999999997, steps: 37, value: -0.4284331798553467\n",
      "reward: 0.89, steps: 12, value: -0.05688232183456421\n",
      "reward: 0.44999999999999973, steps: 46, value: -0.32392919063568115\n",
      "reward: 0.7899999999999999, steps: 22, value: 0.43387091159820557\n",
      "reward: 0.9, steps: 11, value: 0.4475846588611603\n",
      "reward: 0.3999999999999997, steps: 51, value: 0.1907186508178711\n",
      "reward: 0.35999999999999965, steps: 55, value: 0.15043573081493378\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.6177594661712646\n",
      "reward: 0.9, steps: 11, value: 0.9683553576469421\n",
      "reward: 0.13999999999999946, steps: 72, value: 0.2713826596736908\n",
      "reward: -0.7100000000000013, steps: 137, value: -0.22366800904273987\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.5259203910827637\n",
      "reward: 0.14999999999999947, steps: 71, value: -0.5861822366714478\n",
      "reward: 0.7899999999999999, steps: 22, value: -0.24971790611743927\n",
      "reward: -3.699999999999973, steps: 300, value: -0.574202299118042\n",
      "reward: 0.35999999999999965, steps: 55, value: -0.5769059062004089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 0.6599999999999999, steps: 30, value: -0.37845712900161743\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.6188892722129822\n",
      "reward: 0.13999999999999946, steps: 72, value: -0.523840606212616\n",
      "reward: 0.89, steps: 12, value: 0.37458816170692444\n",
      "reward: 0.4199999999999998, steps: 49, value: -0.4440463185310364\n",
      "reward: 0.47999999999999976, steps: 43, value: 0.14970287680625916\n",
      "reward: 0.4399999999999997, steps: 47, value: -0.16012723743915558\n",
      "reward: -0.11000000000000075, steps: 92, value: -0.1258867084980011\n",
      "reward: 0.6299999999999999, steps: 33, value: 0.09494635462760925\n",
      "reward: 0.07999999999999942, steps: 78, value: -0.08211018890142441\n",
      "reward: -0.1500000000000008, steps: 91, value: -0.15974324941635132\n",
      "reward: 0.5699999999999998, steps: 39, value: 0.13449425995349884\n",
      "reward: -0.030000000000000686, steps: 84, value: 0.07810970395803452\n",
      "reward: 0.34999999999999964, steps: 56, value: 0.09237130731344223\n",
      "reward: -3.8499999999999726, steps: 300, value: -0.4748517870903015\n",
      "reward: 0.2699999999999996, steps: 64, value: 0.1705855429172516\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.4822036623954773\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.5089945197105408\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.6109552383422852\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.6743574738502502\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.7213307619094849\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.7546557188034058\n",
      "reward: -0.44000000000000083, steps: 115, value: -0.7330000400543213\n",
      "reward: 0.3899999999999997, steps: 52, value: -0.6292491555213928\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.8980982303619385\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.9038442969322205\n",
      "reward: -0.4700000000000011, steps: 123, value: -0.17376556992530823\n",
      "reward: 0.1699999999999997, steps: 69, value: -0.2758326232433319\n",
      "reward: 0.37999999999999967, steps: 53, value: -0.5667169690132141\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.8200110793113708\n",
      "reward: 0.5099999999999998, steps: 45, value: -0.6289047002792358\n",
      "reward: -3.699999999999973, steps: 300, value: -0.5502660274505615\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.836790144443512\n",
      "reward: 0.91, steps: 10, value: -0.5100547075271606\n",
      "reward: 0.36999999999999966, steps: 54, value: -0.7036097049713135\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.8742476105690002\n",
      "reward: 0.6399999999999999, steps: 32, value: -0.5133764743804932\n",
      "reward: -3.7499999999999742, steps: 300, value: -0.8581541776657104\n",
      "reward: 0.9, steps: 11, value: 0.0025439804885536432\n",
      "reward: 0.3299999999999996, steps: 58, value: -0.6188836097717285\n",
      "reward: 0.34999999999999964, steps: 56, value: -0.2671784460544586\n",
      "reward: 0.46999999999999986, steps: 44, value: -0.31682226061820984\n",
      "reward: 0.91, steps: 10, value: -0.013650099746882915\n",
      "reward: -3.7499999999999747, steps: 300, value: -0.7663194537162781\n",
      "reward: 0.5699999999999998, steps: 39, value: 0.4317452311515808\n",
      "reward: 0.4099999999999997, steps: 50, value: 0.26215860247612\n",
      "reward: -0.11000000000000075, steps: 92, value: -0.06649898737668991\n",
      "reward: 0.6399999999999998, steps: 32, value: 0.6019468307495117\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.5329169034957886\n",
      "reward: 0.1899999999999995, steps: 67, value: 0.011761825531721115\n",
      "reward: 0.04999999999999961, steps: 81, value: 0.05934538692235947\n",
      "reward: 0.3899999999999997, steps: 52, value: -0.20452648401260376\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.45618510246276855\n",
      "reward: 0.08999999999999943, steps: 77, value: 0.19058679044246674\n",
      "reward: 0.6299999999999999, steps: 33, value: 0.3458824157714844\n",
      "reward: 0.9, steps: 11, value: 0.4911642372608185\n",
      "reward: 0.21999999999999953, steps: 64, value: 0.3171241283416748\n",
      "reward: 0.21999999999999953, steps: 64, value: 0.10382373631000519\n",
      "reward: 0.3299999999999996, steps: 58, value: 0.23270735144615173\n",
      "reward: 0.7199999999999999, steps: 24, value: 0.05635497346520424\n",
      "reward: 0.36999999999999966, steps: 54, value: 0.3047716021537781\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.46183905005455017\n",
      "reward: -3.749999999999973, steps: 300, value: -0.4530816674232483\n",
      "reward: 0.88, steps: 13, value: 0.5752708911895752\n",
      "reward: -0.07000000000000071, steps: 88, value: 0.5736979842185974\n",
      "reward: 0.059999999999999616, steps: 80, value: 0.04863574355840683\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.5992115139961243\n",
      "reward: -3.799999999999973, steps: 300, value: -0.47181376814842224\n",
      "reward: -3.8499999999999734, steps: 300, value: -0.49186238646507263\n",
      "reward: 0.91, steps: 10, value: 0.0460578128695488\n",
      "reward: -3.7999999999999727, steps: 300, value: -0.6223710179328918\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.6854692697525024\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.7072887420654297\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.7930060625076294\n",
      "reward: 0.87, steps: 14, value: 0.18967190384864807\n",
      "reward: 0.6399999999999999, steps: 32, value: -0.4960947334766388\n",
      "reward: 0.44999999999999973, steps: 46, value: -0.5271214246749878\n",
      "reward: -3.6999999999999744, steps: 300, value: -0.7672808766365051\n",
      "reward: 0.6699999999999998, steps: 29, value: -0.16193853318691254\n",
      "reward: 0.3699999999999998, steps: 54, value: -0.15767094492912292\n",
      "reward: 0.9, steps: 11, value: 0.5919255018234253\n",
      "reward: -3.849999999999974, steps: 300, value: -0.744733989238739\n",
      "reward: -3.7499999999999742, steps: 300, value: -0.7463622093200684\n",
      "reward: 0.6199999999999999, steps: 34, value: 0.31619134545326233\n",
      "reward: 0.3999999999999997, steps: 51, value: 0.1795714944601059\n",
      "reward: 0.22999999999999954, steps: 63, value: 0.32954463362693787\n",
      "reward: -3.7499999999999747, steps: 300, value: -0.6083621978759766\n",
      "reward: -3.6999999999999735, steps: 300, value: -0.5991491079330444\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.64227694272995\n",
      "reward: 0.35999999999999965, steps: 55, value: -0.3102816045284271\n",
      "reward: 0.89, steps: 12, value: 0.5584443807601929\n",
      "reward: 0.3999999999999997, steps: 51, value: -0.08168048411607742\n",
      "reward: -3.6999999999999744, steps: 300, value: -0.735578179359436\n",
      "reward: 0.9, steps: 11, value: 0.9932634830474854\n",
      "reward: 0.37999999999999967, steps: 53, value: 0.36839985847473145\n",
      "reward: -3.799999999999973, steps: 300, value: -0.7267547845840454\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.6561982035636902\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.6573293805122375\n",
      "reward: 0.5299999999999998, steps: 38, value: 0.04672132059931755\n",
      "reward: 0.4299999999999997, steps: 48, value: -0.06020227447152138\n",
      "reward: -3.6999999999999718, steps: 300, value: -0.6682945489883423\n",
      "reward: 0.5999999999999999, steps: 36, value: 0.40570971369743347\n",
      "reward: -3.799999999999973, steps: 300, value: -0.6778860092163086\n",
      "reward: 0.9, steps: 11, value: 0.8912808895111084\n",
      "reward: 0.45999999999999974, steps: 45, value: 0.167549267411232\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.6351025700569153\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.6647137999534607\n",
      "reward: -0.4100000000000008, steps: 112, value: 0.49103498458862305\n",
      "reward: 0.36999999999999966, steps: 54, value: 0.046000078320503235\n",
      "reward: 0.3899999999999997, steps: 52, value: 0.027097750455141068\n",
      "reward: 0.9, steps: 11, value: 0.1776142567396164\n",
      "reward: 0.88, steps: 13, value: 0.450422078371048\n",
      "reward: -0.040000000000000695, steps: 90, value: 0.2102300375699997\n",
      "reward: 0.6199999999999999, steps: 34, value: 0.22028648853302002\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.6580376029014587\n",
      "reward: 0.91, steps: 10, value: 0.19652238488197327\n",
      "reward: -3.649999999999974, steps: 300, value: -0.6358733773231506\n",
      "reward: -3.7499999999999725, steps: 300, value: -0.672180712223053\n",
      "reward: -3.699999999999973, steps: 300, value: -0.6915549039840698\n",
      "reward: -3.699999999999973, steps: 300, value: -0.708430290222168\n",
      "reward: 0.33999999999999964, steps: 57, value: -0.15571868419647217\n",
      "reward: 0.91, steps: 10, value: 0.1395498663187027\n",
      "reward: 0.4199999999999998, steps: 49, value: -0.058285459876060486\n",
      "reward: 0.23999999999999955, steps: 62, value: -0.05242127925157547\n",
      "reward: 0.4199999999999997, steps: 49, value: 0.09999917447566986\n",
      "reward: -3.749999999999973, steps: 300, value: -0.7046512961387634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 0.5299999999999998, steps: 38, value: 0.43971920013427734\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.6699020266532898\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.6473867893218994\n",
      "reward: 0.4199999999999998, steps: 49, value: 0.21971575915813446\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.5926439762115479\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.6100403070449829\n",
      "reward: -3.7999999999999727, steps: 300, value: -0.6265756487846375\n",
      "reward: 0.16999999999999948, steps: 69, value: 0.06637505441904068\n",
      "reward: 0.1999999999999995, steps: 66, value: 0.31346991658210754\n",
      "reward: -3.7999999999999727, steps: 300, value: -0.616132378578186\n",
      "reward: 0.4099999999999997, steps: 50, value: 0.26447609066963196\n",
      "reward: 0.3099999999999997, steps: 60, value: 0.45906686782836914\n",
      "reward: -3.6999999999999735, steps: 300, value: -0.6543443202972412\n",
      "reward: -3.699999999999974, steps: 300, value: -0.6617374420166016\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.6437176465988159\n",
      "reward: 0.6099999999999999, steps: 35, value: 1.4079103469848633\n",
      "reward: 0.34999999999999964, steps: 56, value: 0.802221953868866\n",
      "reward: 0.6199999999999999, steps: 34, value: 0.9626979827880859\n",
      "reward: 0.33999999999999964, steps: 57, value: 0.9424073696136475\n",
      "reward: 0.4999999999999998, steps: 41, value: 0.5615547299385071\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.5286862254142761\n",
      "reward: 0.15999999999999948, steps: 70, value: 0.27953481674194336\n",
      "reward: -3.699999999999974, steps: 300, value: -0.6045370697975159\n",
      "reward: -3.649999999999974, steps: 300, value: -0.5852313041687012\n",
      "reward: -3.6999999999999744, steps: 300, value: -0.655390739440918\n",
      "reward: -0.10000000000000052, steps: 91, value: -0.11205819249153137\n",
      "reward: -0.400000000000001, steps: 111, value: -0.1745491921901703\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.6575022339820862\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.6653805375099182\n",
      "reward: -3.849999999999974, steps: 300, value: -0.6506491303443909\n",
      "reward: 0.5299999999999998, steps: 38, value: 0.23053070902824402\n",
      "reward: 0.89, steps: 12, value: 0.5274115204811096\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.6677425503730774\n",
      "reward: -0.18000000000000083, steps: 99, value: -0.05323332920670509\n",
      "reward: 0.47999999999999976, steps: 43, value: 0.3605594336986542\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.6349467635154724\n",
      "reward: -3.7499999999999725, steps: 300, value: -0.6416580677032471\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.657008707523346\n",
      "reward: 0.23999999999999955, steps: 62, value: 0.14886733889579773\n",
      "reward: 0.91, steps: 10, value: 0.48628443479537964\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.6707002520561218\n",
      "reward: 0.33999999999999964, steps: 57, value: 0.3114743232727051\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.6761331558227539\n",
      "reward: -3.799999999999973, steps: 300, value: -0.6923394203186035\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.7002579569816589\n",
      "reward: 0.6699999999999998, steps: 29, value: 0.5715746283531189\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.6693474054336548\n",
      "reward: 0.3999999999999997, steps: 51, value: 0.3307986855506897\n",
      "reward: 0.1999999999999995, steps: 66, value: 0.25453823804855347\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.7146123647689819\n",
      "reward: 0.91, steps: 10, value: 0.7839747071266174\n",
      "reward: 0.6699999999999998, steps: 29, value: 0.6219932436943054\n",
      "reward: 0.33999999999999964, steps: 52, value: 0.393067866563797\n",
      "reward: -3.799999999999973, steps: 300, value: -0.7024648189544678\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.6881331205368042\n",
      "reward: 0.7199999999999999, steps: 24, value: 0.3175775110721588\n",
      "reward: 0.85, steps: 16, value: 0.44306012988090515\n",
      "reward: 0.01999999999999936, steps: 84, value: 0.07596482336521149\n",
      "reward: 0.22999999999999954, steps: 63, value: 0.389618843793869\n",
      "reward: -3.699999999999974, steps: 300, value: -0.6915278434753418\n",
      "reward: 0.9, steps: 11, value: 0.8808250427246094\n",
      "reward: 0.88, steps: 13, value: 0.9933050870895386\n",
      "reward: -3.749999999999972, steps: 300, value: -0.6815611720085144\n",
      "reward: -3.749999999999972, steps: 300, value: -0.6992152333259583\n",
      "reward: 0.22999999999999954, steps: 63, value: -0.11760514974594116\n",
      "reward: 0.2699999999999996, steps: 59, value: 0.2355823963880539\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.7687661647796631\n",
      "reward: 0.2899999999999996, steps: 62, value: 0.3361930251121521\n",
      "reward: 0.9, steps: 11, value: 0.7205774784088135\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.7410075664520264\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.7522901892662048\n",
      "reward: 0.91, steps: 10, value: 0.5761283040046692\n",
      "reward: 0.5099999999999998, steps: 40, value: 0.7864352464675903\n",
      "reward: 0.7, steps: 26, value: 0.7645840048789978\n",
      "reward: -3.749999999999972, steps: 300, value: -0.723295271396637\n",
      "reward: 0.46999999999999975, steps: 44, value: 0.7040415406227112\n",
      "reward: -3.6999999999999744, steps: 300, value: -0.7466928958892822\n",
      "reward: 0.1899999999999995, steps: 67, value: 0.005615920759737492\n",
      "reward: 0.22999999999999965, steps: 68, value: 0.1256978064775467\n",
      "reward: 0.2699999999999996, steps: 59, value: 0.1762179285287857\n",
      "reward: -3.6999999999999744, steps: 300, value: -0.6758095026016235\n",
      "reward: 0.88, steps: 13, value: 0.5008304715156555\n",
      "reward: -3.7499999999999747, steps: 300, value: -0.6815911531448364\n",
      "reward: -3.899999999999973, steps: 300, value: -0.6869726777076721\n",
      "reward: -3.799999999999973, steps: 300, value: -0.7155974507331848\n",
      "reward: 0.6799999999999999, steps: 28, value: 0.42235898971557617\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.7410479784011841\n",
      "reward: -3.7999999999999723, steps: 300, value: -0.7135946750640869\n",
      "reward: -0.21000000000000085, steps: 97, value: 0.23067720234394073\n",
      "reward: -3.7999999999999745, steps: 300, value: -0.7031934261322021\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.7257805466651917\n",
      "reward: 0.45999999999999974, steps: 45, value: 0.305921733379364\n",
      "reward: 0.7899999999999999, steps: 22, value: 0.12062546610832214\n",
      "reward: -0.1600000000000008, steps: 92, value: 0.3376828730106354\n",
      "reward: 0.4299999999999997, steps: 48, value: 0.40053117275238037\n",
      "reward: 0.15999999999999948, steps: 70, value: 0.33167555928230286\n",
      "reward: 0.5899999999999999, steps: 37, value: 0.3787977397441864\n",
      "reward: 0.89, steps: 12, value: 0.3112618029117584\n",
      "reward: 0.86, steps: 15, value: 0.46198025345802307\n",
      "reward: -3.6999999999999744, steps: 300, value: -0.6456724405288696\n",
      "reward: 0.91, steps: 10, value: 0.46604451537132263\n",
      "reward: -3.649999999999974, steps: 300, value: -0.6610718369483948\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.7015513181686401\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.6986747980117798\n",
      "reward: -3.699999999999973, steps: 300, value: -0.7245467305183411\n",
      "reward: -3.6999999999999744, steps: 300, value: -0.708693265914917\n",
      "reward: 0.3299999999999996, steps: 53, value: 0.005793645512312651\n",
      "reward: 0.33999999999999975, steps: 57, value: -0.21421416103839874\n",
      "reward: 0.3299999999999996, steps: 58, value: 0.2415141463279724\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.7539071440696716\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.7435925006866455\n",
      "reward: 0.6399999999999998, steps: 32, value: 0.4699462652206421\n",
      "reward: 0.69, steps: 27, value: 0.5203734636306763\n",
      "reward: 0.6399999999999999, steps: 32, value: 0.42148885130882263\n",
      "reward: 0.3999999999999997, steps: 51, value: 0.344455748796463\n",
      "reward: -3.6999999999999744, steps: 300, value: -0.7368378639221191\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.7138211131095886\n",
      "reward: -3.699999999999973, steps: 300, value: -0.6831265091896057\n",
      "reward: 0.6399999999999999, steps: 32, value: 0.48018932342529297\n",
      "reward: -3.6999999999999744, steps: 300, value: -0.6593986749649048\n",
      "reward: 0.6299999999999998, steps: 33, value: 0.475081205368042\n",
      "reward: 0.4299999999999997, steps: 48, value: 0.308440238237381\n",
      "reward: 0.45999999999999974, steps: 45, value: 0.09500564634799957\n",
      "reward: 0.9, steps: 11, value: 0.4989050030708313\n",
      "reward: 0.3299999999999996, steps: 58, value: 0.10492692142724991\n",
      "reward: 0.91, steps: 10, value: 0.6016131639480591\n",
      "reward: 0.049999999999999385, steps: 81, value: 0.08607272803783417\n",
      "reward: 0.6699999999999998, steps: 29, value: 0.6114999651908875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: -3.799999999999973, steps: 300, value: -0.6876853704452515\n",
      "reward: 0.35999999999999965, steps: 55, value: 0.6777551174163818\n",
      "reward: 0.10999999999999945, steps: 75, value: 0.42226168513298035\n",
      "reward: 0.029999999999999367, steps: 78, value: 0.421672523021698\n",
      "reward: 0.15999999999999948, steps: 70, value: 0.4167543351650238\n",
      "reward: 0.1999999999999995, steps: 66, value: -0.1683834195137024\n",
      "reward: -3.649999999999973, steps: 300, value: -0.6173906326293945\n",
      "reward: 0.91, steps: 10, value: 0.5081078410148621\n",
      "reward: 0.34999999999999964, steps: 56, value: -0.4291916489601135\n",
      "reward: 0.5499999999999998, steps: 41, value: 0.2169971913099289\n",
      "reward: -3.749999999999974, steps: 300, value: -0.7616782784461975\n",
      "reward: -3.699999999999973, steps: 300, value: -0.7162759304046631\n",
      "reward: -0.13000000000000078, steps: 94, value: 0.2972310185432434\n",
      "reward: 0.7, steps: 26, value: 0.39146628975868225\n",
      "reward: 0.89, steps: 12, value: 0.6049656271934509\n",
      "reward: -3.699999999999973, steps: 300, value: -0.6558117866516113\n",
      "reward: -3.7999999999999736, steps: 300, value: -0.689150869846344\n",
      "reward: 0.2899999999999997, steps: 62, value: 0.4027545154094696\n",
      "reward: 0.91, steps: 10, value: -0.3420591950416565\n",
      "reward: 0.89, steps: 12, value: 0.8995938301086426\n",
      "reward: 0.91, steps: 10, value: 0.7482558488845825\n",
      "reward: -3.799999999999973, steps: 300, value: -0.6406641602516174\n",
      "reward: 0.4099999999999997, steps: 50, value: 0.551554262638092\n",
      "reward: 0.3299999999999996, steps: 53, value: 0.47144559025764465\n",
      "reward: 0.37999999999999967, steps: 53, value: 0.4271957576274872\n",
      "reward: -3.7499999999999734, steps: 300, value: -0.6362839937210083\n",
      "reward: 0.4099999999999998, steps: 50, value: 0.48522940278053284\n",
      "reward: 0.46999999999999975, steps: 44, value: -0.5906752943992615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-11:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tingyumao/Programs/anaconda3/envs/dlenv/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/tingyumao/Programs/anaconda3/envs/dlenv/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-6-7c9e243535de>\", line 34, in <lambda>\n",
      "    worker_work = lambda: worker.work(max_episode_length,gamma,sess,coord,saver)\n",
      "  File \"<ipython-input-4-38a9273314a9>\", line 109, in work\n",
      "    r = self.env.make_action(self.actions[a]) / 100.0\n",
      "vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.\n",
      "\n",
      "Exception in thread Thread-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tingyumao/Programs/anaconda3/envs/dlenv/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/tingyumao/Programs/anaconda3/envs/dlenv/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-6-7c9e243535de>\", line 34, in <lambda>\n",
      "    worker_work = lambda: worker.work(max_episode_length,gamma,sess,coord,saver)\n",
      "  File \"<ipython-input-4-38a9273314a9>\", line 109, in work\n",
      "    r = self.env.make_action(self.actions[a]) / 100.0\n",
      "vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.\n",
      "\n",
      "Exception in thread Thread-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tingyumao/Programs/anaconda3/envs/dlenv/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/tingyumao/Programs/anaconda3/envs/dlenv/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-6-7c9e243535de>\", line 34, in <lambda>\n",
      "    worker_work = lambda: worker.work(max_episode_length,gamma,sess,coord,saver)\n",
      "  File \"<ipython-input-4-38a9273314a9>\", line 109, in work\n",
      "    r = self.env.make_action(self.actions[a]) / 100.0\n",
      "vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.\n",
      "\n",
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tingyumao/Programs/anaconda3/envs/dlenv/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/tingyumao/Programs/anaconda3/envs/dlenv/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-6-7c9e243535de>\", line 34, in <lambda>\n",
      "    worker_work = lambda: worker.work(max_episode_length,gamma,sess,coord,saver)\n",
      "  File \"<ipython-input-4-38a9273314a9>\", line 109, in work\n",
      "    r = self.env.make_action(self.actions[a]) / 100.0\n",
      "vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "    \n",
    "#Create a directory to save episode playback gifs to\n",
    "if not os.path.exists('./frames'):\n",
    "    os.makedirs('./frames')\n",
    "\n",
    "with tf.device(\"/cpu:0\"): \n",
    "    global_episodes = tf.Variable(0,dtype=tf.int32,name='global_episodes',trainable=False)\n",
    "    trainer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "    master_network = AC_Network(s_size,a_size,'global',None) # Generate global network\n",
    "    num_workers = multiprocessing.cpu_count() # Set workers to number of available CPU threads\n",
    "    workers = []\n",
    "    # Create worker classes\n",
    "    for i in range(num_workers):\n",
    "        workers.append(Worker(DoomGame(),i,s_size,a_size,trainer,model_path,global_episodes))\n",
    "    saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    if load_model == True:\n",
    "        print ('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    # This is where the asynchronous magic happens.\n",
    "    # Start the \"work\" process for each worker in a separate threat.\n",
    "    worker_threads = []\n",
    "    for worker in workers:\n",
    "        worker_work = lambda: worker.work(max_episode_length,gamma,sess,coord,saver)\n",
    "        t = threading.Thread(target=(worker_work))\n",
    "        t.start()\n",
    "        sleep(0.5)\n",
    "        worker_threads.append(t)\n",
    "    coord.join(worker_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
